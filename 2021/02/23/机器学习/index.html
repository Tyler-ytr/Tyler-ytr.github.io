<!DOCTYPE html>
<html>
<head>
    

    

    



    <meta charset="utf-8">
    
    
    
    
    <title>机器学习导论笔记 | Tyler-yin&#39;s blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
    <meta name="theme-color" content="#3F51B5">
    
    
    <meta name="keywords" content="ML">
    <meta name="description" content="机器学习笔记感谢毕秋宇同学！！ Chapter 2空间 假设空间  假设满足XX条件的是好瓜   版本空间  有限训练集，已知XX是好瓜   归纳偏好  假设空间和训练集一致的假设 学习过程中对某种类型假设的偏好称为归纳偏好   No Free Lunch 奥卡姆剃刀：两个模型效果同样好，选择较为简单的    模型评估与选择 经验误差与过拟合 错误率率&amp;误差 错误率：错份样本的占$E&#x3D;a&#x2F;">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习导论笔记">
<meta property="og:url" content="http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="Tyler-yin&#39;s blog">
<meta property="og:description" content="机器学习笔记感谢毕秋宇同学！！ Chapter 2空间 假设空间  假设满足XX条件的是好瓜   版本空间  有限训练集，已知XX是好瓜   归纳偏好  假设空间和训练集一致的假设 学习过程中对某种类型假设的偏好称为归纳偏好   No Free Lunch 奥卡姆剃刀：两个模型效果同样好，选择较为简单的    模型评估与选择 经验误差与过拟合 错误率率&amp;误差 错误率：错份样本的占$E&#x3D;a&#x2F;">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-02-23T09:26:04.000Z">
<meta property="article:modified_time" content="2021-03-22T02:14:34.402Z">
<meta property="article:author" content="Tyler-yin">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
    
        <link rel="alternate" type="application/atom+xml" title="Tyler-yin&#39;s blog" href="/atom.xml">
    
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="/css/style.css?v=1.7.2">
    <script>window.lazyScripts=[]</script>

    <!-- custom head -->
    

<meta name="generator" content="Hexo 4.2.1"></head>

<body>
    <div id="loading" class="active"></div>

    <aside id="menu" class="hide" >
  <div class="inner flex-row-vertical">
    <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menu-off">
        <i class="icon icon-lg icon-close"></i>
    </a>
    <div class="brand-wrap" style="background-image:url(/img/brand.jpg)">
      <div class="brand">
        <a href="/" class="avatar waves-effect waves-circle waves-light">
          <img src="/img/Tyler.png">
        </a>
        <hgroup class="introduce">
          <h5 class="nickname">Tyler-yin</h5>
          <a href="mailto:ytrpossible@gmail.com" title="ytrpossible@gmail.com" class="mail">ytrpossible@gmail.com</a>
        </hgroup>
      </div>
    </div>
    <div class="scroll-wrap flex-col">
      <ul class="nav">
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-home"></i>
                Index
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/archives"  >
                <i class="icon icon-lg icon-archives"></i>
                Archives
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/tags"  >
                <i class="icon icon-lg icon-tags"></i>
                Tags
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/categories"  >
                <i class="icon icon-lg icon-th-list"></i>
                Categories
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="https://github.com/Tyler-ytr/" target="_blank" >
                <i class="icon icon-lg icon-github"></i>
                Github
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/friends"  >
                <i class="icon icon-lg icon-address-book"></i>
                Friends
              </a>
            </li>
        
            <li class="waves-block waves-effect">
              <a href="/"  >
                <i class="icon icon-lg icon-link"></i>
                link
              </a>
            </li>
        
      </ul>
    </div>
  </div>
</aside>

    <main id="main">
        <header class="top-header" id="header">
    <div class="flex-row">
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light on" id="menu-toggle">
          <i class="icon icon-lg icon-navicon"></i>
        </a>
        <div class="flex-col header-title ellipsis">机器学习导论笔记</div>
        
        <div class="search-wrap" id="search-wrap">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="back">
                <i class="icon icon-lg icon-chevron-left"></i>
            </a>
            <input type="text" id="key" class="search-input" autocomplete="off" placeholder="Search">
            <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="search">
                <i class="icon icon-lg icon-search"></i>
            </a>
        </div>
        
        
        <a href="javascript:;" class="header-icon waves-effect waves-circle waves-light" id="menuShare">
            <i class="icon icon-lg icon-share-alt"></i>
        </a>
        
    </div>
</header>
<header class="content-header post-header">

    <div class="container fade-scale">
        <h1 class="title">机器学习导论笔记</h1>
        <h5 class="subtitle">
            
                <time datetime="2021-02-23T09:26:04.000Z" itemprop="datePublished" class="page-time">
  2021-02-23
</time>


	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a></li></ul>

            
        </h5>
    </div>

    


</header>


<div class="container body-wrap">
    
    <aside class="post-widget">
        <nav class="post-toc-wrap post-toc-shrink" id="post-toc">
            <h4>TOC</h4>
            <ol class="post-toc"><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#机器学习笔记"><span class="post-toc-number">1.</span> <span class="post-toc-text">机器学习笔记</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter-2"><span class="post-toc-number">2.</span> <span class="post-toc-text">Chapter 2</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#空间"><span class="post-toc-number">2.1.</span> <span class="post-toc-text">空间</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#模型评估与选择"><span class="post-toc-number">2.2.</span> <span class="post-toc-text">模型评估与选择</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter-3"><span class="post-toc-number">3.</span> <span class="post-toc-text">Chapter 3</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#基本形式"><span class="post-toc-number">3.1.</span> <span class="post-toc-text">基本形式</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#线性回归"><span class="post-toc-number">3.2.</span> <span class="post-toc-text">线性回归</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#二分类问题"><span class="post-toc-number">3.3.</span> <span class="post-toc-text">二分类问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#多分类问题"><span class="post-toc-number">3.4.</span> <span class="post-toc-text">多分类问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter4-决策树"><span class="post-toc-number">4.</span> <span class="post-toc-text">Chapter4 决策树</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-1-基本流程"><span class="post-toc-number">4.1.</span> <span class="post-toc-text">4.1 基本流程</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#决策树基于树结构来进行预测"><span class="post-toc-number">4.1.0.1.</span> <span class="post-toc-text">决策树基于树结构来进行预测</span></a></li><li class="post-toc-item post-toc-level-4"><a class="post-toc-link" href="#树结构的return"><span class="post-toc-number">4.1.0.2.</span> <span class="post-toc-text">树结构的return</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#1-当前节点包含的样本全部属于同一类别-没必要分类"><span class="post-toc-number">4.1.0.2.1.</span> <span class="post-toc-text">(1)当前节点包含的样本全部属于同一类别(没必要分类)</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#2-当前的属性集为空-或所有样本所有属性上取值相同-没法分类"><span class="post-toc-number">4.1.0.2.2.</span> <span class="post-toc-text">(2)当前的属性集为空,或所有样本所有属性上取值相同(没法分类)</span></a></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#3-当前节点包含的样本集合为空-emptyset"><span class="post-toc-number">4.1.0.2.3.</span> <span class="post-toc-text">(3)当前节点包含的样本集合为空($ \emptyset $)</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-2划分选择"><span class="post-toc-number">4.2.</span> <span class="post-toc-text">4.2划分选择</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#划分选择-信息增益"><span class="post-toc-number">4.2.0.0.1.</span> <span class="post-toc-text">划分选择-信息增益</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#信息熵"><span class="post-toc-number">4.2.0.0.1.1.</span> <span class="post-toc-text">信息熵</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#信息增益"><span class="post-toc-number">4.2.0.0.1.2.</span> <span class="post-toc-text">信息增益</span></a></li><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#存在的问题"><span class="post-toc-number">4.2.0.0.1.3.</span> <span class="post-toc-text">存在的问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#划分选择-增益率"><span class="post-toc-number">4.2.0.0.2.</span> <span class="post-toc-text">划分选择-增益率</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-6"><a class="post-toc-link" href="#存在的问题-1"><span class="post-toc-number">4.2.0.0.2.1.</span> <span class="post-toc-text">存在的问题</span></a></li></ol></li><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#划分选择-基尼指数"><span class="post-toc-number">4.2.0.0.3.</span> <span class="post-toc-text">划分选择-基尼指数</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-3剪枝处理"><span class="post-toc-number">4.3.</span> <span class="post-toc-text">4.3剪枝处理</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-4连续与缺失值"><span class="post-toc-number">4.4.</span> <span class="post-toc-text">4.4连续与缺失值</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-5"><a class="post-toc-link" href="#连续与缺失值-连续值处理"><span class="post-toc-number">4.4.0.0.1.</span> <span class="post-toc-text">连续与缺失值-连续值处理</span></a></li></ol></li></ol></li></ol></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#4-5多变量决策树"><span class="post-toc-number">4.5.</span> <span class="post-toc-text">4.5多变量决策树</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter-5"><span class="post-toc-number">5.</span> <span class="post-toc-text">Chapter 5</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-1-神经网络模型与发展史"><span class="post-toc-number">5.1.</span> <span class="post-toc-text">5.1 神经网络模型与发展史</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-2-感知机与多层网络"><span class="post-toc-number">5.2.</span> <span class="post-toc-text">5.2 感知机与多层网络</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-3-误差逆传播算法"><span class="post-toc-number">5.3.</span> <span class="post-toc-text">5.3 误差逆传播算法</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-4-全局最小与局部极小"><span class="post-toc-number">5.4.</span> <span class="post-toc-text">5.4 全局最小与局部极小</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#5-5-其他常见神经网络"><span class="post-toc-number">5.5.</span> <span class="post-toc-text">5.5 其他常见神经网络</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter-6-支持向量机"><span class="post-toc-number">6.</span> <span class="post-toc-text">Chapter 6: 支持向量机</span></a></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#传奇——一刀999"><span class="post-toc-number">7.</span> <span class="post-toc-text">传奇——一刀999</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-1-间隔与支持向量"><span class="post-toc-number">7.1.</span> <span class="post-toc-text">6.1 间隔与支持向量</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-2-对偶问题"><span class="post-toc-number">7.2.</span> <span class="post-toc-text">6.2 对偶问题</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-3-核函数"><span class="post-toc-number">7.3.</span> <span class="post-toc-text">6.3 核函数</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-4-软间隔与正则化"><span class="post-toc-number">7.4.</span> <span class="post-toc-text">6.4 软间隔与正则化</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-5-支持向量回归"><span class="post-toc-number">7.5.</span> <span class="post-toc-text">6.5 支持向量回归</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#6-6-核方法"><span class="post-toc-number">7.6.</span> <span class="post-toc-text">6.6 核方法</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chapter-7-贝叶斯"><span class="post-toc-number">8.</span> <span class="post-toc-text">Chapter 7 :贝叶斯</span></a><ol class="post-toc-child"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-1-贝叶斯决策论"><span class="post-toc-number">8.1.</span> <span class="post-toc-text">7.1 贝叶斯决策论</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-2极大似然估计"><span class="post-toc-number">8.2.</span> <span class="post-toc-text">7.2极大似然估计</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-3-朴素贝叶斯分类器-naive-Bayes-classifier"><span class="post-toc-number">8.3.</span> <span class="post-toc-text">7.3 朴素贝叶斯分类器(naive Bayes classifier)</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-4-半朴素贝叶斯分类器"><span class="post-toc-number">8.4.</span> <span class="post-toc-text">7.4 半朴素贝叶斯分类器</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#7-5-贝叶斯网"><span class="post-toc-number">8.5.</span> <span class="post-toc-text">7.5 贝叶斯网</span></a></li></ol></li><li class="post-toc-item post-toc-level-1"><a class="post-toc-link" href="#Chap-8-集成学习"><span class="post-toc-number">9.</span> <span class="post-toc-text">Chap 8 集成学习</span></a></li></ol>
        </nav>
    </aside>


<article id="post-机器学习"
  class="post-article article-type-post fade" itemprop="blogPost">

    <div class="post-card">
        <h1 class="post-card-title">机器学习导论笔记</h1>
        <div class="post-meta">
            <time class="post-time" title="2021-02-23 17:26:04" datetime="2021-02-23T09:26:04.000Z"  itemprop="datePublished">2021-02-23</time>

            
	<ul class="article-category-list"><li class="article-category-list-item"><a class="article-category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0/">课程笔记</a></li></ul>



            
<span id="busuanzi_container_page_pv" title="文章总阅读量" style='display:none'>
    <i class="icon icon-eye icon-pr"></i><span id="busuanzi_value_page_pv"></span>
</span>


        </div>
        <div class="post-content" id="post-content" itemprop="postContent">
            <h1 id="机器学习笔记"><a href="#机器学习笔记" class="headerlink" title="机器学习笔记"></a>机器学习笔记</h1><p>感谢毕秋宇同学！！</p>
<h1 id="Chapter-2"><a href="#Chapter-2" class="headerlink" title="Chapter 2"></a>Chapter 2</h1><h2 id="空间"><a href="#空间" class="headerlink" title="空间"></a>空间</h2><ul>
<li><p>假设空间</p>
<ul>
<li>假设满足XX条件的是好瓜</li>
</ul>
</li>
<li><p>版本空间</p>
<ul>
<li>有限训练集，已知XX是好瓜</li>
</ul>
</li>
<li><p>归纳偏好</p>
<ul>
<li>假设空间和训练集一致的假设</li>
<li>学习过程中对某种类型假设的偏好称为归纳偏好</li>
</ul>
</li>
<li>No Free Lunch<ul>
<li>奥卡姆剃刀：两个模型效果同样好，选择较为简单的</li>
</ul>
</li>
</ul>
<h2 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h2><ul>
<li>经验误差与过拟合<ul>
<li>错误率率&amp;误差<ul>
<li>错误率：错份样本的占$E=a/m$</li>
<li>误差：样本真实输出与预测输出之间的差异<ul>
<li>训练（经验）误差：训练集上</li>
<li>测试误差：测试集</li>
<li>泛化误差：初训练集外所有样本</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>过拟合<ul>
<li>学习器把训练样本学习的“太好”，将训练样本本身的特点当作所有样本的一般性质，导致泛化性能下降</li>
<li>优化目标加正则项</li>
<li>Early stop</li>
</ul>
</li>
<li>欠拟合<ul>
<li>对训练样本的一般性质尚未学好</li>
<li>决策树：扩展分支</li>
<li>神经网络：增加训练层数</li>
</ul>
</li>
<li>评估方法<ul>
<li>留出法<ul>
<li>直接将数据集划分为两个互斥集合</li>
<li>训练/测试集划分要尽可能保持数据分布的一致性</li>
<li>一般若干次随机划分，重复实验取平均值</li>
<li>训练/测试样本比例通常为2:1～4:1</li>
</ul>
</li>
<li>交叉验证法<ul>
<li>将数据集分层采样划分为$k$个大小相似的互斥子集</li>
</ul>
</li>
<li>自助法<ul>
<li>以自助采样法为基础，对数据集$D$有放回采样$m$次得到训练集$D^{\prime}$，$D\backslash D^{\prime}$用作测试集</li>
</ul>
</li>
</ul>
</li>
<li>性能度量<ul>
<li>性能度量是衡量模型泛化能力的评价标准，反映任务的需求<ul>
<li>回归任务最常用的是“均方误差”：<ul>
<li>$E(f:D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^{2}$</li>
</ul>
</li>
</ul>
</li>
<li>查准率 $P=\frac{TP}{TP+FP}$</li>
<li>查全率 $R=\frac{TP}{TP+FN}$</li>
<li>$P-R$曲线</li>
<li>$F1$ measure：$\frac{2\times TP}{N+TP-TN}$</li>
<li>$AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_{i})\cdot(y_{i}+y_{i+1})$，预测了排序质量</li>
<li>代价敏感错误率</li>
</ul>
</li>
<li>性能评估<ul>
<li>关于性能比较<ul>
<li>测试性能并不等于泛化性能</li>
<li>测试性能随着测试集的变化而变化</li>
<li>很多机器学习算法本身有一定的随机性</li>
<li>直接选取相应评估方式在相应条件下评估并不可靠</li>
</ul>
</li>
<li>二项检验<ul>
<li>泛化错误率为$\epsilon$，测试错误率为$\hat{\epsilon}$，嘉定测试样本从样本总体分布中独立采样而来，我们可以使用“二项检验”，对于$\epsilon&lt;epsilon_{0}$进行假设检验。</li>
<li>假设$\epsilon\leq\epsilon_{0}$，若测试错误率小于</li>
</ul>
</li>
<li>$t$检验</li>
<li>交叉验证$t$检验</li>
</ul>
</li>
<li>偏差和方差<ul>
<li>对于测试样本$x$，令$y_{D}$为$x$在数据集中的标记，$y$为$x$的真实标记，$f(x;D)$为训练集$D$上学的模型$f$在$x$上的预测输出。</li>
<li>以回归任务为例：<ul>
<li>期望预期为：$\bar{f}(x)=\mathbb{E}_{D}[f(x;D)]$；</li>
<li>使用样本数目相同的不同训练集产生的方差为$var(x)=\mathbb{E}_{D}[(f(x:D)-\bar{f}(x))^{2}]$；</li>
<li>噪声为$\varepsilon^{2}=\mathbb{E}<em>{D}[(y</em>{D}-y)^{2}]$</li>
</ul>
</li>
<li>$E(f;D)=bias^{2}(x)+var(x)+\varepsilon^2$</li>
</ul>
</li>
</ul>
<h1 id="Chapter-3"><a href="#Chapter-3" class="headerlink" title="Chapter 3"></a>Chapter 3</h1><h2 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h2><ul>
<li>线性模型一般形式$f(x)=w_1x_1+w_2x_2+\cdots+w_dx_d+b$</li>
<li><p>向量形式$f(x)=w^{T}x+b$</p>
</li>
<li><p>区分猫狗的例子</p>
<ul>
<li>按照像素行堆叠或列堆叠，成为一个向量</li>
<li>乘以单位，对于二分类问题，单位就是一个向量</li>
</ul>
</li>
<li>Perceptron感知机<ul>
<li>对于线性分类器，误分类则$-y_{1}(w\cdot x_i)+b&gt;0$</li>
<li>定义损失函数$L(w,b)=-\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)$</li>
<li>梯度$\bigtriangledown_{w}L(w,b)=-\sum_{x_{i}\in M}y_{i}x_{i}$</li>
<li>$\bigtriangledown_{b}L(w,b)=-\sum_{x_{i}\in M}y_{i}$</li>
</ul>
</li>
<li>梯度下降法<ul>
<li>一阶方法</li>
<li>考虑无约束优化$min_{x}f(x)$，$f(x+\Delta x)\approx f(x)+\Delta x^{T}\bigtriangledown f(x)$</li>
<li>$\Delta x^{T}\bigtriangledown f(x)&lt;0$</li>
<li>$\Delta x=-\gamma \bigtriangledown f(x)$</li>
<li>$\gamma$使用二分查找法进行查找</li>
</ul>
</li>
<li>优点<ul>
<li>形式简单，易于建模</li>
<li>可解释性</li>
<li>非线性模型的基础<ul>
<li>引入层级结构或高维映射</li>
</ul>
</li>
</ul>
</li>
<li>缺陷<ul>
<li>解决不了$x^{2}$问题</li>
</ul>
</li>
</ul>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ul>
<li><p>目的：学得一个线性模型以尽可能准确地预测实值输出标记</p>
</li>
<li><p>离散属性处理</p>
<ul>
<li>有“序”关系<ul>
<li>连续化为连续值</li>
</ul>
</li>
<li>无“序”关系</li>
</ul>
</li>
<li>单一属性的线性回归目标<ul>
<li>$f(x)=wx_{i}+b$使得$f(x_{i})\simeq y_{i}$</li>
</ul>
</li>
<li><p>参数/模型估计：最小二乘法（Least square method）</p>
<ul>
<li>$(w^{<em>},b^{</em>})=arg\ min_{(w,b)}\sum_{i=1}^{m}(f(x_{i})-y_i)^2$</li>
<li>最小化均方误差$E_{(w,b)}$</li>
</ul>
</li>
<li><p>多元线性回归</p>
<ul>
<li>$f(\hat{x_{i}})=\hat{x_{i}}^{T}(X^{T}X)^{-1}$</li>
<li>$X^{T}X$不满秩，进行正则化</li>
</ul>
</li>
<li>广义线性模型<ul>
<li>一般形式：$y=g^{-1}(w^{T}x+b)$<ul>
<li>$g$为联系函数(link function)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="二分类问题"><a href="#二分类问题" class="headerlink" title="二分类问题"></a>二分类问题</h2><ul>
<li>预测值与输出标记$z=w^{T}x+b$</li>
<li>寻找函数将分类标记与线性回归模型输出联系起来</li>
<li>最理想的模型——单位阶跃函数</li>
<li><p>替代函数——对数几率函数（logistic function）</p>
<ul>
<li><strong>$y=\frac{1}{1+e^{-z}}$</strong></li>
<li>运行对数几率函数$y=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(w^{T}x+b)}}$</li>
<li>对数几率</li>
</ul>
</li>
<li><p>样本作为正例的相对可能性的对数$\ln\frac{y}{1-y}=\ln\frac{p(y=1|x)}{p(y=0|x)}=w^{T}x+b$</p>
</li>
<li><p>极大似然法</p>
</li>
<li><p>给定数据集${(x_{i}, y_{i})}^{m}_{i=1}$</p>
</li>
<li><p>最大化样本属于其真实标记的概率</p>
</li>
<li><p>线性判别分析（Linear Discriminant Analysis）</p>
<ul>
<li>最大化目标$J=\frac{|w^{T}\mu_{0}-w^{T}\mu_{1}|^{2}<em>{2}}{w^{T}\sum</em>{0}w+w^{T}\sum_{1}w}=\frac{w^{T}(\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}w}{w^{T}(\sum_{0}+\sum_{1})w}$</li>
<li>类间散度矩阵，类内散度矩阵</li>
<li>广义瑞丽商$J=\frac{w^{T}S_{b}w}{w^{T}S_{w}w}$</li>
<li>拉格朗日乘子法<ul>
<li>$\bigtriangledown f(x^{<em>})+\lambda \Delta g(x^{</em>})=0$</li>
<li>$L(x,\lambda)=f(x)+\lambda g(x)$.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><ul>
<li>多分类学习方法<ul>
<li>二分类学习方法推广到多类</li>
<li>利用二分类学习器解决多分类问题<ul>
<li>对问题进行拆分，为拆出的每个二分类任务训练一个分类器</li>
<li>对于每个分类器的预测结果进行集成以获得最终的多分类结果</li>
</ul>
</li>
<li>拆分策略<ul>
<li>一对一（OVO）<ul>
<li>$N$个类别两两配对，$N(N-1)/2$个二类任务</li>
<li>各个二类任务学习分类器，$N(N-1)/2$个二类分类器</li>
</ul>
</li>
<li>一对其余（OVR）</li>
<li>多对多（MVM）<ul>
<li>若干类作为正类，若干类作为反类</li>
<li>输出纠错码（Error Correcting Output Code, ECOC）</li>
</ul>
</li>
</ul>
</li>
<li>类别不平衡问题$(class\ imbalance)$<ul>
<li>不同类别训练样例数相差很大情况（正类为小类）</li>
<li>类别平衡正例预测$\frac{y}{1-y}&gt;1\Rightarrow \frac{y}{1-y}&gt;\frac{m^{+}}{m^{-}}$正负类比例</li>
</ul>
</li>
<li>再缩放<ul>
<li>欠采样$(undersampling)$<ul>
<li>去除一些反例使正反例数目接近</li>
</ul>
</li>
<li>过采样$(oversampling)$<ul>
<li>增加一些正例使正反例数目接近</li>
</ul>
</li>
<li>阈值移动$(threshold-moving)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="Chapter4-决策树"><a href="#Chapter4-决策树" class="headerlink" title="Chapter4 决策树"></a>Chapter4 决策树</h1><h2 id="4-1-基本流程"><a href="#4-1-基本流程" class="headerlink" title="4.1 基本流程"></a>4.1 基本流程</h2><h4 id="决策树基于树结构来进行预测"><a href="#决策树基于树结构来进行预测" class="headerlink" title="决策树基于树结构来进行预测"></a>决策树基于树结构来进行预测</h4><ul>
<li><p>如果用决策树来进行分来,起码该模型一定意义上是可以理解的</p>
<h4 id="树结构的return"><a href="#树结构的return" class="headerlink" title="树结构的return"></a>树结构的return</h4><h5 id="1-当前节点包含的样本全部属于同一类别-没必要分类"><a href="#1-当前节点包含的样本全部属于同一类别-没必要分类" class="headerlink" title="(1)当前节点包含的样本全部属于同一类别(没必要分类)"></a>(1)当前节点包含的样本全部属于同一类别(没必要分类)</h5><h5 id="2-当前的属性集为空-或所有样本所有属性上取值相同-没法分类"><a href="#2-当前的属性集为空-或所有样本所有属性上取值相同-没法分类" class="headerlink" title="(2)当前的属性集为空,或所有样本所有属性上取值相同(没法分类)"></a>(2)当前的属性集为空,或所有样本所有属性上取值相同(没法分类)</h5><h5 id="3-当前节点包含的样本集合为空-emptyset"><a href="#3-当前节点包含的样本集合为空-emptyset" class="headerlink" title="(3)当前节点包含的样本集合为空($ \emptyset $)"></a>(3)当前节点包含的样本集合为空($ \emptyset $)</h5><h2 id="4-2划分选择"><a href="#4-2划分选择" class="headerlink" title="4.2划分选择"></a>4.2划分选择</h2></li>
<li><p>希望决策树的分支节点包含的样本尽可能属于同一类别,即节点的”纯度”(purity)越来越高</p>
</li>
<li><p>经典的属性划分方法:<br>1)信息增益,<br>2)增益率,<br>3)基尼指数</p>
<h5 id="划分选择-信息增益"><a href="#划分选择-信息增益" class="headerlink" title="划分选择-信息增益"></a>划分选择-信息增益</h5></li>
<li><p>“信息熵”是度量样本集合纯度最常用的一种指标</p>
<h6 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h6></li>
<li><p>$$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$</p>
</li>
<li><p>推导:$$\int P(x)f(x)dx\rightarrow E_{x - p}(f(x))\rightarrow E_{x-p} (log_2 \frac{1}{p_k})$$</p>
<ul>
<li><p>(log2可以表示用二进制表示,$\frac{1}{p_k}$可以显示信息(概率越低越刺激))</p>
</li>
<li><p>计算信息熵的约定:若p=0,则Ent=0</p>
</li>
<li><p>Ent(D)的值越小,纯度越大</p>
<h6 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h6></li>
</ul>
</li>
<li><p>$$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v})$$</p>
</li>
<li><p>算出信息增益之后,可以确定树的每一层应该对应哪些划分属性</p>
<h6 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h6></li>
<li><p>信息增益对可取值数目较多的属性有所偏好</p>
<h5 id="划分选择-增益率"><a href="#划分选择-增益率" class="headerlink" title="划分选择-增益率"></a>划分选择-增益率</h5></li>
<li><p>增益率：$$Gain_ ratio (D,a)=\frac{Gain(D,a)}{IV(a)}$$<br>其中    $$IV(a)=-\sum _{v=1}^{V}\frac{D^v}{D}log_2 \frac{D^v}{D}$$</p>
<h6 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h6></li>
<li><p>增益率准则对可取值数目较少的属性有所偏好</p>
<h5 id="划分选择-基尼指数"><a href="#划分选择-基尼指数" class="headerlink" title="划分选择-基尼指数"></a>划分选择-基尼指数</h5><p> $$ Gini(D)=\sum_{k=1}^{|y|}\sum _{k’\neq k}p_kp_{k’}=1-\sum_{k=1}^{|y|}p_k^{2}$$</p>
</li>
<li><p>Gini越小纯度越高</p>
</li>
</ul>
<h2 id="4-3剪枝处理"><a href="#4-3剪枝处理" class="headerlink" title="4.3剪枝处理"></a>4.3剪枝处理</h2><ul>
<li><p>为了对抗过拟合</p>
</li>
<li><p>基本策略</p>
<ul>
<li>预剪枝</li>
<li>后剪枝</li>
</ul>
</li>
<li><p>判断决策树泛化性能是否提升的办法</p>
<ul>
<li>留出法<br>#####预剪枝</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>降低过拟合风险</li>
<li>显著减少训练时间和测试时间开销</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>欠拟合风险<br>#####后剪枝</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>比预剪枝保留了更多的分支,欠拟合风险小,泛化性能往往由于预剪枝决策树</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>时间开销<h2 id="4-4连续与缺失值"><a href="#4-4连续与缺失值" class="headerlink" title="4.4连续与缺失值"></a>4.4连续与缺失值</h2><h5 id="连续与缺失值-连续值处理"><a href="#连续与缺失值-连续值处理" class="headerlink" title="连续与缺失值-连续值处理"></a>连续与缺失值-连续值处理</h5></li>
</ul>
</li>
<li><p>连续属性离散化(二分法)</p>
</li>
<li><p>缺失值处理</p>
<ul>
<li>面临两个问题 如何划分,如何测试</li>
</ul>
</li>
</ul>
<h2 id="4-5多变量决策树"><a href="#4-5多变量决策树" class="headerlink" title="4.5多变量决策树"></a>4.5多变量决策树</h2><h1 id="Chapter-5"><a href="#Chapter-5" class="headerlink" title="Chapter 5"></a>Chapter 5</h1><h2 id="5-1-神经网络模型与发展史"><a href="#5-1-神经网络模型与发展史" class="headerlink" title="5.1 神经网络模型与发展史"></a>5.1 神经网络模型与发展史</h2><ul>
<li>第一阶段<ul>
<li>M-P模型</li>
<li>Hebb学习规则:类似于巴普洛夫,if input and output 同时激活或者失活, 那么这两个神经元的链接应该被加强,else 应该减弱</li>
<li>感知机网络</li>
<li>自适应神经元,最小均方学习算法</li>
<li>GG 1: 单层的神经网络不能解决非线性问题,多层神经网络算力不足的时候就输了</li>
</ul>
</li>
<li>第二阶段<ul>
<li>Hopfield 网络</li>
<li>反向传播算法</li>
<li>SVM与统计学习理论</li>
</ul>
</li>
<li>第三阶段<ul>
<li>DBN深度信念网络</li>
</ul>
</li>
<li>神经元模型<ul>
<li>M-P神经元模型<ul>
<li>输入:来自其他n个神经元传递过来的输入信号</li>
<li>处理:输入信号通过带权重的连接进行传递,神经元接收到的总输入值将与神经元的阈值进行比较</li>
<li>输出:通过激活函数的处理以得到输出:$y=f(\sum _{i=1}^{n}w_ix_i-<br>\theta)$</li>
<li>扯一点signmoid(x)=$\frac{1}{1+e^{-x}}$<br>$y’=\frac{1}{1+e^{-x}}’=\frac{1}{1+e^{-x}}\times \frac{e^{-x}}{1+e^{-x}}=y(1-y)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-2-感知机与多层网络"><a href="#5-2-感知机与多层网络" class="headerlink" title="5.2 感知机与多层网络"></a>5.2 感知机与多层网络</h2><ul>
<li>感知机<ul>
<li>由两层神经元组成,输入层接受外界输入信号传递个输出层,输出层是M-神经元</li>
<li>与或非</li>
<li>多层感知机</li>
<li>多层前馈神经网络<ul>
<li>定义:每层神经元与下一层神经元全互联</li>
<li>前馈:输入层接受外界输入,隐含层与输出层神经元对信号进行加工,最终结果由输出层神经元输出</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-3-误差逆传播算法"><a href="#5-3-误差逆传播算法" class="headerlink" title="5.3 误差逆传播算法"></a>5.3 误差逆传播算法</h2><ul>
<li>误差逆传播算法(BP)<ul>
<li>前向计算:<ul>
<li>$b_h=f(\beta _h -y_h ),\beta <em>h=\sum</em>{i=1}^{d}v_{ih}x_i$</li>
<li>$\hat{y}_i^{k}=f(a_j-\theta _j),\alpha_h=\sum_{i=1}^{d} w_{hj}b_{h}$ </li>
<li>$E_k=\frac{1}{2}\sum_{j=1}^{l}(\hat{y}<em>{j}^{k}-\hat{y}</em>{j}^{k})^{2}$</li>
</ul>
</li>
<li>参数数目<ul>
<li>权重:$v_{ih},w_{hj}$, 阈值:$\theta <em>{j}$,$y</em>{h}$ (i=1,…,d,h=1,…,q,j=1,…,l)<br>因此网络中需要(d+l+1)q+l个参数需要优化</li>
</ul>
</li>
<li>参数优化<ul>
<li>BP是迭代学习算法,在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计,任意的参数v的更新估计式为<br>$$v\leftarrow v+\Delta v$$</li>
<li>梯度咋整? 梯度消失咋整?</li>
<li>因为本质上还是函数的复合,因此不能用线性函数做传递函数</li>
</ul>
</li>
</ul>
</li>
<li>一些BP算法<ul>
<li>标准BP算法:每来一个样本,就扔到bp网络中,然后前馈得到误差,得到误差之后就逆传递回来更新网络……</li>
<li>累计BP算法:用平均误差更新权值,一定意义上可以减少震荡</li>
<li>实际应用: Mini BP 一块一块更新</li>
</ul>
</li>
<li>多层前馈网络:<ul>
<li>局限:<ul>
<li>可能遭遇过拟合</li>
<li>到底搞几层？</li>
</ul>
</li>
<li>解决过拟合策略：<ul>
<li>正则化</li>
<li>早停</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="5-4-全局最小与局部极小"><a href="#5-4-全局最小与局部极小" class="headerlink" title="5.4 全局最小与局部极小"></a>5.4 全局最小与局部极小</h2><ul>
<li>策略<ul>
<li>使用”模拟退火”</li>
<li>随机梯度下降</li>
<li>遗传算法 </li>
</ul>
</li>
</ul>
<h2 id="5-5-其他常见神经网络"><a href="#5-5-其他常见神经网络" class="headerlink" title="5.5 其他常见神经网络"></a>5.5 其他常见神经网络</h2><ul>
<li>RBF网络:单隐层,激活函数与输入向量有关:<br>$$\phi(x)=\sum^{q}_{i=1}w_i\rho(x,c_i)$$<br>$\rho(x,c_i)$是径向基函数：<br>$$\rho(x,c_i)=e^{-\beta_i ||x-c_i||^{2}}$$</li>
<li>ART网络：自适应谐振理论<br>竞争性学习网络,输出神经元互相竞争,遵循胜者通吃原则<br>比较层,识别层(???)</li>
<li>SOM网络：获取数据内在的结构</li>
<li>级联相关,或则隐层节点的构造</li>
<li>Elman网络：递归神经网络 </li>
</ul>
<h1 id="Chapter-6-支持向量机"><a href="#Chapter-6-支持向量机" class="headerlink" title="Chapter 6: 支持向量机"></a>Chapter 6: 支持向量机</h1><h1 id="传奇——一刀999"><a href="#传奇——一刀999" class="headerlink" title="传奇——一刀999"></a>传奇——一刀999</h1><p>神经网络(1989-1994)(BP算法)———–&gt;支持向量机 (1995-2005)(核方法,统计学习)(Vapnik)———&gt; 神经网络(2006-今)(深度学习)</p>
<table>
<thead>
<tr>
<th>支持向量机</th>
<th style="text-align:left">灵活（核方法）</th>
<th>能力很强</th>
<th>数学理论坚实</th>
<th>全局最优解</th>
<th>不需要人工调参</th>
</tr>
</thead>
<tbody>
<tr>
<td>神经网络</td>
<td style="text-align:left">更灵活</td>
<td>能力很强</td>
<td>理论不清,来自认知</td>
<td>局部最优解</td>
<td>非常依赖人工调参</td>
</tr>
<tr>
<td>支持向量机(SVM)</td>
<td style="text-align:left">计算开销大</td>
<td>领域知识嵌入困难(对现象的认识)</td>
<td>服务于科学界</td>
<td></td>
<td></td>
</tr>
<tr>
<td>神经网络(NN)</td>
<td style="text-align:left">可大可小</td>
<td>领域知识无处不在</td>
<td>服务于工业界</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="6-1-间隔与支持向量"><a href="#6-1-间隔与支持向量" class="headerlink" title="6.1 间隔与支持向量"></a>6.1 间隔与支持向量</h2><ul>
<li><p>最大间隔: 寻找参数$w$,b,使得$\nu=\frac{2}{||w||}$ 最大 </p>
<ul>
<li><p>$$arg~max_{w,b}~\frac{2}{||w||}$$    </p>
<p>s.t. $y_i(w^{T}x_i+b)\geq 1,i=1,2,…,m$</p>
</li>
<li><p>$$arg~min~\frac{1}{2}||w||^{2}$$</p>
<p>s.t. $y_i(w^{T}x_i+b) \geq 1,i=1,2,…,m$</p>
</li>
</ul>
</li>
</ul>
<h2 id="6-2-对偶问题"><a href="#6-2-对偶问题" class="headerlink" title="6.2 对偶问题"></a>6.2 对偶问题</h2><ul>
<li><p>拉格朗日乘子法</p>
<ul>
<li>三步</li>
</ul>
</li>
<li><p>解的稀疏性</p>
<ul>
<li><p>最终模型:$f(x)=w^Tx+b=\sum_{i=1}^{m}a_iy_ix_{i}^{T}x+b$</p>
</li>
<li><p>KKT条件:</p>
<p>$${<br>\begin{cases}<br>a_i\geq 0\<br>y_if(x_i)\geq 1\a_i(y_if(x_i)-1)=0\end{cases}}$$</p>
</li>
</ul>
</li>
<li><p>SMO-求解方法,选两个固定搞，因为两个有闭式解；</p>
</li>
</ul>
<h2 id="6-3-核函数"><a href="#6-3-核函数" class="headerlink" title="6.3 核函数"></a>6.3 核函数</h2><ul>
<li><p>从低维映射到高维可以用线性的方式进行分类，只要维数足够高，高维的空间总可以线性的来分类；</p>
</li>
<li><p>将x映射到$\phi(x)$转换前面的最终模型为:$f(x)=w^Tx+b=\sum_{i=1}^{m}a_iy_i\phi(x)_{i}^{T}\phi(x)+b$</p>
<p>定义核函数:$k(x_i,x_j)=\phi(x_i)^{T}\phi(x_j)$</p>
<p>定义核矩阵:$$K= {\left[  \begin{matrix}    k(x_1,x_1) &amp; k(x_1,x_2) &amp; … &amp; k(x_1,x_n)\  k(x_2,x_1) &amp; k(x_2,x_2) &amp; … &amp; k(x_2,y_n) \   …&amp;… &amp; … &amp; ..  \   k(x_n,x_1) &amp; k(x_n,x_2) &amp; … &amp; k(x_n,y_n)  \end{matrix}  \right]} $$</p>
</li>
</ul>
<h2 id="6-4-软间隔与正则化"><a href="#6-4-软间隔与正则化" class="headerlink" title="6.4 软间隔与正则化"></a>6.4 软间隔与正则化</h2><ul>
<li>黑人问号，谜の调参(C)</li>
</ul>
<h2 id="6-5-支持向量回归"><a href="#6-5-支持向量回归" class="headerlink" title="6.5 支持向量回归"></a>6.5 支持向量回归</h2><ul>
<li>因为听不懂所以自闭了 感觉大概推了一个神奇的二次回归吧</li>
</ul>
<h2 id="6-6-核方法"><a href="#6-6-核方法" class="headerlink" title="6.6 核方法"></a>6.6 核方法</h2><ul>
<li>表示定理：对于任意单调增函数$\Omega$和任意非负损失函数l,优化问题……</li>
</ul>
<h1 id="Chapter-7-贝叶斯"><a href="#Chapter-7-贝叶斯" class="headerlink" title="Chapter 7 :贝叶斯"></a>Chapter 7 :贝叶斯</h1><h2 id="7-1-贝叶斯决策论"><a href="#7-1-贝叶斯决策论" class="headerlink" title="7.1 贝叶斯决策论"></a>7.1 贝叶斯决策论</h2><p>概率框架下实施决策的基本理论：</p>
<ul>
<li><p>条件风险</p>
<ul>
<li>$R(c_i|x)=\sum_{j=1}^{N}\lambda _{ij}P(c_j|x)$</li>
</ul>
</li>
<li><p>贝叶斯判定准则</p>
<ul>
<li>$h^{*}(x)=argmin_{c\in \gamma} ~R(c|x)$</li>
<li>$h^*(x)$:贝叶斯最优分类器,</li>
<li>反应了机器学习所能产生的模型精度的理论上限</li>
</ul>
</li>
<li><p>P(c|x)在现实中通常难以直接获得</p>
</li>
<li><p>两种基本策略：</p>
<ul>
<li>判别式模型：直接对P(c|x)建模<ul>
<li>决策树</li>
<li>BP神经网络</li>
<li>SVM</li>
</ul>
</li>
<li>生成式模型：先对联合概率分布P(x,c)建模,再由此获得P(c|x)<ul>
<li>$P(c|x)=\frac{P(x.c)}{P(x)}$</li>
</ul>
</li>
</ul>
</li>
<li><p>贝叶斯定理</p>
<ul>
<li><p>后验概率与联合概率分布的关系：</p>
<p>$P(c|x)=\frac{P=(x,c)}{P(x)}$</p>
</li>
<li><p>$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)P(x|c)}{\int P(c)P(x|c)~dc}$</p>
<p>P(c):先验概率(人群中得病概率为1/1000000)</p>
<p>P(x):证据</p>
<p>P(c|x):后验概率(医院看到了症状之后判断的概率)  </p>
<p>P(x|c): likehood 似然(重点在另外的变量$\theta$上) ,likehood function目的是求$\theta$ </p>
<p>Prob(重点在变量上面) </p>
</li>
</ul>
</li>
</ul>
<h2 id="7-2极大似然估计"><a href="#7-2极大似然估计" class="headerlink" title="7.2极大似然估计"></a>7.2极大似然估计</h2><p>fenleiqifenleiqi（不太懂 <a href="https://blog.csdn.net/chenjianbo88/article/details/52398181" target="_blank" rel="noopener">https://blog.csdn.net/chenjianbo88/article/details/52398181</a></p>
<h2 id="7-3-朴素贝叶斯分类器-naive-Bayes-classifier"><a href="#7-3-朴素贝叶斯分类器-naive-Bayes-classifier" class="headerlink" title="7.3 朴素贝叶斯分类器(naive Bayes classifier)"></a>7.3 朴素贝叶斯分类器(naive Bayes classifier)</h2><p>假定属性独立地对分类结果发生影响</p>
<p>$$P(c|x)=\frac{P(c)P(x|c)}{P(x)}=\frac{P(c)}{P(x)}\Pi^{d}_{i=1}P(x_i|c)$$</p>
<ul>
<li>P(c)=$\frac{D_c}{D}$</li>
<li>$P(x_i|c)=\frac{D_{c,x_i}}{D_c}$</li>
<li>拉普拉斯修正(为了解决某一个属性值在训练集中没有与某个类同时出现过就凉了 所以要假设出现了一次)<ul>
<li>$\hat{P(c)}=\frac{|D_c|+1}{|D|+N}$</li>
<li>$\hat{P(x_i|c)}=\frac{|D+{c_i,x_i}|+1}{|D_c|+N_i}$</li>
</ul>
</li>
<li>hint: 连续的分布用概率密度来代替,(一般用高斯分布)</li>
</ul>
<h2 id="7-4-半朴素贝叶斯分类器"><a href="#7-4-半朴素贝叶斯分类器" class="headerlink" title="7.4 半朴素贝叶斯分类器"></a>7.4 半朴素贝叶斯分类器</h2><p>One - Dependent Esitimator</p>
<ul>
<li><p>SPODE</p>
<p>超父带你飞</p>
</li>
<li><p>TAN</p>
<ul>
<li><p>计算条件互信息</p>
<p>$I(x_i,x_j|y)=\sum _{x_i,x_j;c\in \gamma}P(x_i,x_j|c)log\frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}$</p>
</li>
<li><p>建完全图,权重设为上面的I</p>
</li>
<li><p>构建最大带权生成树,挑选根变量,将变置为有向</p>
</li>
<li><p>加入类别节点y,增加从y到每一个属性的有向边</p>
</li>
</ul>
</li>
<li><p>AODE</p>
</li>
</ul>
<h2 id="7-5-贝叶斯网"><a href="#7-5-贝叶斯网" class="headerlink" title="7.5 贝叶斯网"></a>7.5 贝叶斯网</h2><p>（概率论知识反应不能,To be continued</p>
<ul>
<li>V型例子:各位的智商x1 考试难度x2  最后的考试成绩x4,考完结果出来x1,x2就不独立了</li>
<li>道德图(从marriage的梗里面获得)</li>
<li></li>
</ul>
<h1 id="Chap-8-集成学习"><a href="#Chap-8-集成学习" class="headerlink" title="Chap 8 集成学习"></a>Chap 8 集成学习</h1><p>​    </p>
<p>​    </p>
<blockquote>
<p>PS：使用<a href="https://runninggump.github.io/2018/12/05/%E6%88%90%E5%8A%9F%E8%A7%A3%E5%86%B3%E5%9C%A8hexo%E4%B8%AD%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">https://runninggump.github.io/2018/12/05/%E6%88%90%E5%8A%9F%E8%A7%A3%E5%86%B3%E5%9C%A8hexo%E4%B8%AD%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E7%9A%84%E9%97%AE%E9%A2%98/</a> 里面的方法解决了博客的数学公式显示问题</p>
</blockquote>

        </div>

        <blockquote class="post-copyright">
    
    <div class="content">
        
<span class="post-time">
    Last updated: <time datetime="2021-03-22T02:14:34.402Z" itemprop="dateUpdated">2021-03-22 10:14:34</time>
</span><br>


        
        Link：<a href="/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" target="_blank" rel="external">http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</a>
        
    </div>
    
    <footer>
        <a href="http://Tyler-ytr.github.io">
            <img src="/img/Tyler.png" alt="Tyler-yin">
            Tyler-yin
        </a>
    </footer>
</blockquote>

        


        <div class="post-footer">
            
	<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>


            
<div class="page-share-wrap">
    

<div class="page-share" id="pageShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&title=《机器学习导论笔记》 — Tyler-yin's blog&pic=http://Tyler-ytr.github.io/img/Tyler.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&title=《机器学习导论笔记》 — Tyler-yin's blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习导论笔记》 — Tyler-yin's blog&url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&via=http://Tyler-ytr.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>



    <a href="javascript:;" id="shareFab" class="page-share-fab waves-effect waves-circle">
        <i class="icon icon-share-alt icon-lg"></i>
    </a>
</div>



        </div>
    </div>

    
<nav class="post-nav flex-row flex-justify-between">
  
    <div class="waves-block waves-effect prev">
      <a href="/2021/02/23/leetcode1052-m/" id="post-prev" class="post-nav-link">
        <div class="tips"><i class="icon icon-angle-left icon-lg icon-pr"></i> Prev</div>
        <h4 class="title">leetcode1052-m</h4>
      </a>
    </div>
  

  
    <div class="waves-block waves-effect next">
      <a href="/2021/02/23/%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B/" id="post-next" class="post-nav-link">
        <div class="tips">Next <i class="icon icon-angle-right icon-lg icon-pl"></i></div>
        <h4 class="title">数学期望</h4>
      </a>
    </div>
  
</nav>



    











    <!-- Valine Comments -->
    <div class="comments vcomment" id="comments"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//unpkg.com/valine@latest/dist/Valine.min.js"></script>
    <!-- Valine Comments script -->
    <script>
        var GUEST_INFO = ['nick','mail','link'];
        var guest_info = 'nick,mail,link'.split(',').filter(function(item){
          return GUEST_INFO.indexOf(item) > -1
        });
        new Valine({
            el: '#comments',
            notify: 'true' == 'true',
            verify: 'false' == 'true',
            appId: "NoLhoWnmbSW89zV4zc04RPwx-gzGzoHsz",
            appKey: "SSdRGaHcdjoKc7cJJpOlJIqJ",
            avatar: "mm",
            placeholder: "Just go go",
            guest_info: guest_info.length == 0 ? GUEST_INFO : guest_info,
            pageSize: "10"
        })
    </script>
    <!-- Valine Comments end -->










</article>



</div>

        <footer class="footer">
    <div class="top">
        
<p>
    <span id="busuanzi_container_site_uv" style='display:none'>
        总访客数：<span id="busuanzi_value_site_uv"></span>
    </span>
    <span id="busuanzi_container_site_pv" style='display:none'>
        总访问量：<span id="busuanzi_value_site_pv"></span>
    </span>
</p>


        <p>
            
                <span><a href="/atom.xml" target="_blank" class="rss" title="rss"><i class="icon icon-lg icon-rss"></i></a></span>
            
            <span>This blog is licensed under a <a rel="license noopener" href="https://creativecommons.org/licenses/by/4.0/" target="_blank">Creative Commons Attribution 4.0 International License</a>.</span>
        </p>
    </div>
    <div class="bottom">
        <p><span>Tyler-yin &copy; 2015 - 2021</span>
            <span>
                
                Power by <a href="http://hexo.io/" target="_blank">Hexo</a> Theme <a href="https://github.com/yscoder/hexo-theme-indigo" target="_blank">indigo</a>
            </span>
        </p>
    </div>
</footer>

    </main>
    <div class="mask" id="mask"></div>
<a href="javascript:;" id="gotop" class="waves-effect waves-circle waves-light"><span class="icon icon-lg icon-chevron-up"></span></a>



<div class="global-share" id="globalShare">
    <ul class="reset share-icons">
      <li>
        <a class="weibo share-sns" target="_blank" href="http://service.weibo.com/share/share.php?url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&title=《机器学习导论笔记》 — Tyler-yin's blog&pic=http://Tyler-ytr.github.io/img/Tyler.png" data-title="微博">
          <i class="icon icon-weibo"></i>
        </a>
      </li>
      <li>
        <a class="weixin share-sns wxFab" href="javascript:;" data-title="微信">
          <i class="icon icon-weixin"></i>
        </a>
      </li>
      <li>
        <a class="qq share-sns" target="_blank" href="http://connect.qq.com/widget/shareqq/index.html?url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&title=《机器学习导论笔记》 — Tyler-yin's blog&source=" data-title=" QQ">
          <i class="icon icon-qq"></i>
        </a>
      </li>
      <li>
        <a class="facebook share-sns" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" data-title=" Facebook">
          <i class="icon icon-facebook"></i>
        </a>
      </li>
      <li>
        <a class="twitter share-sns" target="_blank" href="https://twitter.com/intent/tweet?text=《机器学习导论笔记》 — Tyler-yin's blog&url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&via=http://Tyler-ytr.github.io" data-title=" Twitter">
          <i class="icon icon-twitter"></i>
        </a>
      </li>
      <li>
        <a class="google share-sns" target="_blank" href="https://plus.google.com/share?url=http://tyler-ytr.github.io/2021/02/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" data-title=" Google+">
          <i class="icon icon-google-plus"></i>
        </a>
      </li>
    </ul>
 </div>


<div class="page-modal wx-share" id="wxShare">
    <a class="close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAAAAAB3DOFrAAACtklEQVR42u3aQU7DQAwFUO5/aZBYIZWm3/Y4Bel1VbUhmTdIY9f2x0f8+vx+/Xz/+Mnjt49XPl7fe+LhFx4eHl5r6dfLSq5JANV7Vjfxl+fi4eHhrfGqS3m2Qc/umW/E9eZW74aHh4f3Xl7+V3liXQ0DeHh4eP+RVy0KzBedrwEPDw/vTl5eFDhwWMeB50BxBA8PD2+BNz/c73+/3t/Dw8PDa3XV86ZUtTF2XdrIg8SLp+Ph4eEt8K6bW0lp4NQ183Grwi8GPDw8vAEvuVEyQDAfHagWQaKkHw8PD2+NNynRJgd03gDLg8f1s/Dw8PD2eL3HJ59Plp60u/JghoeHh7fNS47s/HCvLv0a09xKPDw8vKO8XvN+UiDopdQJ+2l/Dw8PD+8or1pOrQ4W5GMB1e2rBiQ8PDy8DV71+K6WJOa5f6/xhoeHh3eW1yuMJgl3/vlGwMDDw8Pb41Ub/6daVoUFxRsRRT88PDy8Ma96WFeT5s/4lVDz4IGHh4d3Jy8vEPSGq+b1kkKqjYeHh7fGSx6TDwfkifJkpCAKNnh4eHgLvPmQUy/57hUmqmViPDw8vPt5+TBT3uKqbso8zODh4eGd5SUHdzNhjZtV1b/KgwceHh7eBi9n5GNPvXS8mnAnxWU8PDy8Pd7kp/7ZAaxeYIhCFx4eHt4hXq8ykQ8KTHj5HZ5+goeHh3cLLy9GHE55W9++CFR4eHh4C7wkkc2pvaR5Pj4VDWzh4eHhHeJNFj0ZC+j9H8oFXzw8PLwF3qQAMS/pVpPmansMDw8Pb49XDQa95eaLzhtdK0NXeHh4eDEvSXCri8i3YBLBXlyDh4eH91ZeHgBOXT8pXuDh4eH9HV61QHDqoG+m+Hh4eHhrvEkxoneUX1+TD2wVJsvw8PDwDvF6DbBqg783IpCEnHw8Cw8PD2/M+wLXFLtgPYBH+wAAAABJRU5ErkJggg==" alt="微信分享二维码">
</div>




    <script src="//cdn.bootcss.com/node-waves/0.7.4/waves.min.js"></script>
<script>
var BLOG = { ROOT: '/', SHARE: true, REWARD: false };


</script>

<script src="/js/main.min.js?v=1.7.2"></script>


<div class="search-panel" id="search-panel">
    <ul class="search-result" id="search-result"></ul>
</div>
<template id="search-tpl">
<li class="item">
    <a href="{path}" class="waves-block waves-effect">
        <div class="title ellipsis" title="{title}">{title}</div>
        <div class="flex-row flex-middle">
            <div class="tags ellipsis">
                {tags}
            </div>
            <time class="flex-col time">{date}</time>
        </div>
    </a>
</li>
</template>

<script src="/js/search.min.js?v=1.7.2" async></script>



<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" async></script>




<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



<script>
(function() {
    var OriginTitile = document.title, titleTime;
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            document.title = '＞﹏＜';
            clearTimeout(titleTime);
        } else {
            document.title = '~\(≧▽≦)/~';
            titleTime = setTimeout(function() {
                document.title = OriginTitile;
            },2000);
        }
    });
})();
</script>


</body>
</html>
